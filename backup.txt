def get_course_details(url):
    try:
        response = requests.get(url, timeout=10)
        if response.status_code != 200: return None
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # 1. TITRE
        title_tag = soup.find('h1')
        title = title_tag.get_text(strip=True) if title_tag else "Sans titre"
        
        # 2. PLANS D'√âTUDES (M√©thode CSS pr√©cise)
        study_plans = []
        plans_container = soup.find('div', class_='study-plans')
        
        if plans_container:
            buttons = plans_container.find_all('button', class_='collapse-title')
            for btn in buttons:
                span_tag = btn.find('span')
                if not span_tag: continue
                
                # Extraction Section et Niveau
                full_btn_text = btn.get_text(strip=True)
                span_text = span_tag.get_text(strip=True)
                section_name = full_btn_text.replace(span_text, "").strip()
                
                level = "Autre"
                if "Bachelor" in span_text: level = "Bachelor"
                elif "Master" in span_text: level = "Master"
                
                # Extraction Type (avec Regex)
                content_div = btn.find_next_sibling('div', class_='collapse-item')
                course_type = "Inconnu"
                
                if content_div:
                    block_text = content_div.get_text(separator="\n")
                    # Regex pour capturer "Type: obligatoire" ou "Type: mandatory"
                    match = re.search(r"(?i)Type\s*:\s*(.*)", block_text)
                    if match:
                        course_type = match.group(1).strip().lower()
                
                study_plans.append({
                    "full_header": section_name,
                    "niveau": level,
                    "type": course_type
                })

        # Fallback si vide
        if not study_plans:
            study_plans.append({"full_header": "G√©n√©ral", "niveau": "Inconnu", "type": "Inconnu"})

        # 3. CONTENU IA
        main_content = soup.find('div', class_='main-content') or soup.find('main') or soup.body
        relevant_text = ""
        if main_content:
            for junk in main_content.find_all(['nav', 'header', 'footer', 'form', 'script', 'style', 'div.study-plans']):
                junk.decompose()
            full_text = main_content.get_text(separator=" ", strip=True)
            match_start = re.search(r"(R√©sum√©|Summary|Contenu|Content)\s*[:\.]?", full_text, re.IGNORECASE)
            start_index = match_start.start() if match_start else 0
            relevant_text = full_text[start_index:2000]

        return {
            "url": url,
            "titre": title,
            "full_embedding_text": f"{title}. {relevant_text}",
            "plans": study_plans
        }

    except Exception as e:
        print(f"‚ùå Erreur {url}: {e}")
        return None



def main():
    print("üöÄ D√©marrage du SCRAPER FINAL...")
    
    if not os.path.exists(INPUT_URLS_FILE):
        print(f"‚ùå Fichier {INPUT_URLS_FILE} introuvable !")
        return

    with open(INPUT_URLS_FILE, "r", encoding="utf-8") as f:
        urls = [line.strip() for line in f if line.strip()]

    print(f"üéØ {len(urls)} cours √† analyser.")
    cours_data = []
    
    for i, url in enumerate(urls):
        print(f"[{i+1}/{len(urls)}] {url.split('/')[-1][:40]}...")
        details = get_course_details(url)
        if details:
            cours_data.append(details)
        
        if (i+1) % 50 == 0:
            with open(OUTPUT_JSON_FILE, "w", encoding="utf-8") as f:
                json.dump(cours_data, f, ensure_ascii=False, indent=4)
        
        time.sleep(0.1) 

    with open(OUTPUT_JSON_FILE, "w", encoding="utf-8") as f:
        json.dump(cours_data, f, ensure_ascii=False, indent=4)
    print("‚úÖ TERMINE ! Lance maintenant 'python indexer.py'")

if __name__ == "__main__":
    main()



    import json
import chromadb
from sentence_transformers import SentenceTransformer
import os

# --- CONFIGURATION ---
# On pointe vers le fichier final (assure-toi d'avoir lanc√© le scraper complet avec ce nom)
INPUT_FILE = "../data/cours_data_final.json"
DB_PATH = "../depfl_cours_db"
COLLECTION_NAME = "cours_epfl"

def main():
    print(f"üìÇ Lecture du fichier : {INPUT_FILE} ...")
    
    if not os.path.exists(INPUT_FILE):
        print(f"‚ùå Erreur : Le fichier {INPUT_FILE} est introuvable.")
        print("üí° As-tu lanc√© le scraper complet ?")
        return

    with open(INPUT_FILE, "r", encoding="utf-8") as f:
        cours_data = json.load(f)

    print(f"   -> {len(cours_data)} cours bruts trouv√©s.")

    # --- √âTAPE 1 : D√âDOUBLONNAGE ---
    # On garde la logique pour √©viter d'avoir 2x le m√™me cours
    unique_courses = {}
    
    for cours in cours_data:
        titre = cours['titre'].strip()
        
        # On ignore les cours vides
        if not titre or titre == "Sans titre":
            continue
            
        # Si on ne l'a pas encore, on l'ajoute
        if titre not in unique_courses:
            # PLUS DE NETTOYAGE ICI : On fait confiance au scraper !
            unique_courses[titre] = cours
    
    clean_data = list(unique_courses.values())
    print(f"‚ú® Nettoyage termin√© : On passe de {len(cours_data)} √† {len(clean_data)} cours uniques.")

    # --- √âTAPE 2 : PR√âPARATION DU CERVEAU (CHROMA) ---
    print("üß† Chargement du mod√®le IA (SentenceTransformer)...")
    # On garde ce mod√®le, c'est le meilleur compromis vitesse/qualit√©
    model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
    
    chroma_client = chromadb.PersistentClient(path=DB_PATH)
    
    # Reset de la base pour repartir √† z√©ro
    try:
        chroma_client.delete_collection(name=COLLECTION_NAME)
        print("üóëÔ∏è Ancienne base supprim√©e.")
    except:
        pass 

    collection = chroma_client.create_collection(name=COLLECTION_NAME)

    # --- √âTAPE 3 : INDEXATION PAR LOTS (BATCHING) ---
    batch_size = 200
    total = len(clean_data)
    
    print("‚öôÔ∏è Cr√©ation des vecteurs et enregistrement dans la base...")
    
    for i in range(0, total, batch_size):
        batch = clean_data[i : i + batch_size]
        
        ids = [c["url"] for c in batch]
        documents = [c["full_embedding_text"] for c in batch]
        
        # On pr√©pare les m√©tadonn√©es
        # (C'est ici qu'on pourra ajouter le tag 'SHS' plus tard pour le filtre avanc√©)
        metadatas = []
        for c in batch:
            # On r√©cup√®re la liste 'plans', ou une liste vide si elle n'existe pas
            plans_data = c.get("plans", [])
            
            # On transforme cette liste en TEXTE (string) pour que ChromaDB l'accepte
            plans_str = json.dumps(plans_data) 
            
            metadatas.append({
                "titre": c["titre"], 
                "url": c["url"],
                "plans_json": plans_str # On stocke la string JSON ici
            })
        # Vectorisation
        embeddings = model.encode(documents).tolist()
        
        collection.add(
            ids=ids,
            embeddings=embeddings,
            documents=documents,
            metadatas=metadatas
        )
        print(f"   [{min(i + batch_size, total)}/{total}] cours index√©s...")

    print("üéâ SUCC√àS ! Base de donn√©es g√©n√©r√©e avec succ√®s.")
    print("üëâ Tu peux maintenant lancer 'streamlit run app.py'")

if __name__ == "__main__":
    main()




import json
import chromadb
from sentence_transformers import SentenceTransformer
import os

# --- CONFIGURATION ---
INPUT_FILE = "../data/cours_data_final.json"
DB_PATH = "../depfl_cours_db"
COLLECTION_NAME = "cours_epfl"

def load_data(filepath):
    """Charge le JSON et v√©rifie qu'il existe."""
    if not os.path.exists(filepath):
        print(f"‚ùå Erreur : Le fichier {filepath} est introuvable.")
        return None
    
    with open(filepath, "r", encoding="utf-8") as f:
        data = json.load(f)
    print(f"üìÇ {len(data)} cours charg√©s depuis {filepath}.")
    return data

def setup_chroma(db_path, collection_name):
    """Initialise ChromaDB et nettoie l'ancienne base."""
    client = chromadb.PersistentClient(path=db_path)
    
    # Reset complet pour √©viter les doublons lors des tests
    try:
        client.delete_collection(name=collection_name)
        print("üóëÔ∏è Ancienne collection supprim√©e.")
    except:
        pass # Elle n'existait pas, pas grave

    collection = client.create_collection(name=collection_name)
    return collection

def main():
    # 1. Chargement
    cours_data = load_data(INPUT_FILE)
    if not cours_data: return

    # 2. Pr√©paration IA & DB
    print("üß† Chargement du mod√®le IA (patience)...")
    model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
    collection = setup_chroma(DB_PATH, COLLECTION_NAME)

    # 3. Indexation par lots (Batching)
    batch_size = 50 # On peut r√©duire un peu pour voir la progression plus souvent
    total = len(cours_data)
    
    print(f"‚öôÔ∏è D√©marrage de l'indexation de {total} cours...")

    for i in range(0, total, batch_size):
        # On d√©coupe une tranche de cours
        batch = cours_data[i : i + batch_size]
        
        # Pr√©paration des listes pour ChromaDB
        ids = []
        documents = []
        metadatas = []

        for cours in batch:
            # --- CORRECTION DES CLES ICI ---
            # On utilise .get() pour √©viter le crash si une cl√© manque
            url = cours.get("url")
            titre = cours.get("title", "Sans titre") # <-- C'est 'title' maintenant
            texte_ia = cours.get("content", "")      # <-- C'est 'content' maintenant
            meta_list = cours.get("metadata", [])    # <-- C'est 'metadata' maintenant

            if not url or not texte_ia: continue # On saute les cours cass√©s

            ids.append(url)
            documents.append(texte_ia)
            
            # Astuce : Chroma ne stocke pas de listes/dictionnaires dans les m√©tadonn√©es.
            # On doit transformer la liste des sections en une cha√Æne de caract√®res (JSON string)
            meta_json = json.dumps(meta_list, ensure_ascii=False)
            
            metadatas.append({
                "title": titre,
                "url": url,
                "metadata_json": meta_json 
            })

        # Si le batch n'est pas vide, on vectorise et on ajoute
        if documents:
            embeddings = model.encode(documents).tolist()
            collection.add(
                ids=ids,
                documents=documents,
                embeddings=embeddings,
                metadatas=metadatas
            )
        
        print(f"   [{min(i + batch_size, total)}/{total}] trait√©s.")

    print("üéâ Indexation termin√©e ! La base de donn√©es est pr√™te dans le dossier 'depfl_cours_db'.")

if __name__ == "__main__":
    main()


import streamlit as st
import chromadb
from sentence_transformers import SentenceTransformer, CrossEncoder
from rank_bm25 import BM25Okapi
import os
import math
import json

# --- CONFIGURATION ---
DB_PATH = "./epfl_cours_db"
COLLECTION_NAME = "cours_epfl"

st.set_page_config(page_title="EPFL Course Recommender", page_icon="üéì", layout="wide")

# --- TA NOUVELLE LISTE COMPLETE ---
# Note: J'ai ajout√© quelques alias anglais courants (Computer Science, Communication Systems) 
# pour t'assurer des r√©sultats m√™me si le scraper tombe sur la page EN.
PROGRAMMES = {
    "üåê Tout explorer": (None, None),

    # BACHELORS
    "üèõÔ∏è Bachelor Architecture": (["Architecture"], "Bachelor"),
    "üß™ Bachelor Chimie": (["Chimie"], "Bachelor"),
    "‚öóÔ∏è Bachelor Chimie et g√©nie chimique": (["Chimie et g√©nie chimique"], "Bachelor"),
    "üè≠ Bachelor G√©nie chimique": (["G√©nie chimique"], "Bachelor"),
    "üèóÔ∏è Bachelor G√©nie civil": (["G√©nie civil", "Civil Engineering"], "Bachelor"),
    "‚öôÔ∏è Bachelor G√©nie m√©canique": (["G√©nie m√©canique", "Mechanical Engineering"], "Bachelor"),
    "‚ö° Bachelor G√©nie √©lectrique": (["G√©nie √©lectrique", "Electrical and Electronics"], "Bachelor"),
    "üíª Bachelor Informatique": (["Informatique", "Computer Science"], "Bachelor"),
    "üß¨ Bachelor Sciences du vivant": (["Ing√©nierie des sciences du vivant", "Life Sciences"], "Bachelor"),
    "üßÆ Bachelor Math√©matiques": (["Math√©matiques", "Mathematics"], "Bachelor"),
    "üî¨ Bachelor Microtechnique": (["Microtechnique", "Microengineering"], "Bachelor"),
    "‚öõÔ∏è Bachelor Physique": (["Physique", "Physics"], "Bachelor"),
    "üß± Bachelor Mat√©riaux": (["Science et g√©nie des mat√©riaux"], "Bachelor"),
    "üåç Bachelor Environnement": (["Science et ing√©nierie de l'environnement"], "Bachelor"),
    "üì° Bachelor SysCom": (["Syst√®mes de communication", "Communication Systems"], "Bachelor"),

    # MASTERS (J'ai gard√© ta structure exacte)
    "üèõÔ∏è Master Architecture": (["Architecture"], "Master"),
    "üß™ Master Chimie mol√©culaire": (["Chimie mol√©culaire et biologique"], "Master"),
    "üìä Master Data Science": (["Data Science"], "Master"),
    "üèóÔ∏è Master G√©nie civil": (["G√©nie civil", "Civil Engineering"], "Master"),
    "‚öôÔ∏è Master G√©nie m√©canique": (["G√©nie m√©canique", "Mechanical Engineering"], "Master"),
    "‚ò¢Ô∏è Master G√©nie nucl√©aire": (["G√©nie nucl√©aire", "Nuclear Engineering"], "Master"),
    "‚ö° Master G√©nie √©lectrique": (["G√©nie √©lectrique", "Electrical and Electronics"], "Master"),
    "üìú Master Humanit√©s digitales": (["Humanit√©s digitales", "Digital Humanities"], "Master"),
    "üíª Master Informatique": (["Informatique", "Computer Science"], "Master"),
    "üõ°Ô∏è Master Cybersec": (["Informatique - Cybersecurity", "Cyber security"], "Master"),
    "üß¨ Master Sciences du vivant": (["Ing√©nierie des sciences du vivant", "Life Sciences"], "Master"),
    "üí∞ Master Ing√©nierie financi√®re": (["Ing√©nierie financi√®re", "Financial Engineering"], "Master"),
    "üßÆ Master Ing√©nierie math√©matique": (["Ing√©nierie math√©matique"], "Master"),
    "‚öõÔ∏è Master Ing√©nierie physique": (["Ing√©nierie physique"], "Master"),
    "üå± Master Management durable": (["Management durable et technologie"], "Master"),
    "üöÄ Master Management & Tech": (["Management, technologie et entrepreneuriat"], "Master"),
    "üìê Master Math√©matiques": (["Math√©matiques"], "Master"),
    "üî¨ Master Micro-Nanotech": (["Micro- and Nanotechnologies"], "Master"),
    "üî¨ Master Microtechnique": (["Microtechnique", "Microengineering"], "Master"),
    "üß† Master Neuro-X": (["Neuro-X"], "Master"),
    "‚öõÔ∏è Master Physique": (["Physique"], "Master"),
    "ü§ñ Master Robotique": (["Robotique", "Robotics"], "Master"),
    "üß± Master Mat√©riaux": (["Science et g√©nie des mat√©riaux"], "Master"),
    "üíª Master Computational Science": (["Science et ing√©nierie computationnelles"], "Master"),
    "üåå Master Quantique": (["Science et ing√©nierie quantiques"], "Master"),
    "üîã Master Energie": (["Science et technologie de l'√©nergie", "Energy"], "Master"),
    "üåç Master Environnement": (["Sciences et ing√©nierie de l'environnement"], "Master"),
    "üìà Master Statistique": (["Statistique"], "Master"),
    "üì° Master SysCom": (["Syst√®mes de communication", "Communication Systems"], "Master"),
    "üèôÔ∏è Master Syst√®mes urbains": (["Syst√®mes urbains"], "Master"),
}

# --- CHARGEMENT ---
def sigmoid(x): return 1 / (1 + math.exp(-(x + 6)))

@st.cache_resource
def load_models():
    return SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2'), CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

@st.cache_resource
def load_db_collection():
    return chromadb.PersistentClient(path=DB_PATH).get_collection(name=COLLECTION_NAME)

@st.cache_resource
def init_bm25(documents):
    return BM25Okapi([doc.split(" ") for doc in documents])

try:
    with st.spinner("Chargement..."):
        sentence_transformer, model_reranker = load_models()
        collection = load_db_collection()
        all_docs = collection.get()
        documents_list = all_docs['documents']
        ids_list = all_docs['ids']
        metadatas_list = all_docs['metadatas']
        bm25_engine = init_bm25(documents_list)
except Exception as e:
    st.error(f"Erreur DB : {e}")
    st.stop()

# --- INTERFACE ---
with st.sidebar:
    st.header("üë§ Votre Profil")
    
    # 1. Le s√©lecteur de programme
    selected_program_name = st.selectbox("Programme", options=list(PROGRAMMES.keys()), index=0) # index 0 pour "Tout explorer" par d√©faut
    target_aliases, target_level = PROGRAMMES[selected_program_name]
    
    st.markdown("---")
    st.header("‚öôÔ∏è Param√®tres de recherche")
    top_k = st.slider("Nombre de r√©sultats max", min_value=5, max_value=100, value=10, step=5)    # 2. Section Debug am√©lior√©e


st.title("üéì EPFL Course Recommender")
job_offer = st.text_area("üìã Ce que vous cherchez...", height=100)
search_btn = st.button("üöÄ Trouver les cours", type="primary", use_container_width=True)

# --- LOGIQUE ---
if search_btn and job_offer:
    with st.spinner("Recherche..."):
        # 1. Retrieval
        query_vector = sentence_transformer.encode(job_offer).tolist()
        
        # ON UTILISE top_k * 2 pour avoir de la marge pour le filtrage
        limit_search = top_k * 2 
        
        v_results = collection.query(query_embeddings=[query_vector], n_results=limit_search)
        v_ids = v_results['ids'][0] if v_results['ids'] else []
        
        bm25_top = bm25_engine.get_top_n(job_offer.split(" "), documents_list, n=limit_search)
        bm25_ids = [ids_list[documents_list.index(d)] for d in bm25_top if d in documents_list]
        
        all_ids = list(set(v_ids + bm25_ids))
        candidates, final_ids, final_metas = [], [], []

        # 2. Filtering
        for doc_id in all_ids:
            try:
                idx = ids_list.index(doc_id)
                meta = metadatas_list[idx]
                plans = json.loads(meta.get('plans_json', '[]'))
                
                keep = False
                badge = ""
                
                if target_aliases is None: # Tout explorer
                    keep = True
                else:
                    # V√©rification LISTE alias
                    for plan in plans:
                        p_head = plan.get('full_header', '').lower()
                        p_lvl = plan.get('niveau', '').lower()
                        
                        # On v√©rifie si UN des alias (ex: "informatique" OU "computer science") est dans le header
                        match_section = any(alias.lower() in p_head for alias in target_aliases)
                        match_level = target_level.lower() in p_lvl
                        
                        if match_section and match_level:
                            keep = True
                            badge = plan.get('type', 'Inconnu')
                            break
                
                if keep:
                    if "Summer workshop" in meta['titre']: continue
                    candidates.append([job_offer, documents_list[idx]])
                    final_ids.append(doc_id)
                    meta['badge'] = badge
                    final_metas.append(meta)
            except: continue

        # 3. Reranking & Display
        if candidates:
            scores = model_reranker.predict(candidates)
            ranked = sorted(zip(scores, final_ids, final_metas, candidates), key=lambda x: x[0], reverse=True)
            
            st.success(f"{len(ranked)} cours trouv√©s")
            for i in range(min(top_k, len(ranked))):
                score, did, meta, content = ranked[i]
                badge_txt = meta.get('badge', '')
                color = "red" if "bligatoire" in badge_txt or "andatory" in badge_txt else "green"
                
                st.markdown(f"### [{meta['titre']}]({did})")
                if badge_txt: st.caption(f":{color}[{badge_txt}]")
                st.progress(sigmoid(score), text=f"Pertinence: {sigmoid(score):.0%}")
                with st.expander("D√©tails"):
                    st.write(content[1][:400]+"...")
                st.divider()
        else:
            st.warning("Aucun cours trouv√©.")


import streamlit as st
import chromadb
from sentence_transformers import SentenceTransformer, CrossEncoder
from rank_bm25 import BM25Okapi

# Constantes
DB_PATH = "./epfl_cours_db"
COLLECTION_NAME = "cours_epfl"

# Cache pour ne pas recharger l'IA √† chaque clic (Hyper important)
@st.cache_resource
def load_resources():
    # 1. Embedder (Recherche vectorielle)
    embedder = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
    
    # 2. Reranker (Juge de pertinence final)
    reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
    
    # 3. Base de donn√©es
    client = chromadb.PersistentClient(path=DB_PATH)
    collection = client.get_collection(name=COLLECTION_NAME)
    
    # 4. Chargement pour BM25 (Mots-cl√©s exacts)
    # On doit tout r√©cup√©rer en m√©moire pour BM25 (c'est rapide pour 2000 cours)
    all_docs = collection.get()
    bm25 = BM25Okapi([doc.split() for doc in all_docs['documents']])
    
    return embedder, reranker, collection, bm25, all_docs

    import json

def search_courses( ):
    # 1. RETRIEVAL (R√©cup√©ration large)
    # A. Vectoriel (Sens)
    q_vec = embedder.encode(query).tolist()
    vec_res = collection.query(query_embeddings=[q_vec], n_results=top_k * 2)
    vec_ids = set(vec_res['ids'][0])

    # B. BM25 (Mots-cl√©s)
    # On r√©cup√®re les indices des documents
    bm25_top = bm25.get_top_n(query.split(), all_data['documents'], n=top_k * 2)
    bm25_ids = set()
    for doc in bm25_top:
        idx = all_data['documents'].index(doc)
        bm25_ids.add(all_data['ids'][idx])

    # Fusion des candidats
    candidates_ids = list(vec_ids.union(bm25_ids))

    # 2. FILTRAGE (Logique m√©tier)
    filtered_candidates = []
    
    target_aliases, target_level = program_filter # Ex: (['Informatique'], 'Master')

    for cid in candidates_ids:
        idx = all_data['ids'].index(cid)
        meta = all_data['metadatas'][idx]
        
        # On d√©code le JSON string des m√©tadonn√©es
        plans = json.loads(meta.get('metadata', '[]')) # Attention √† la cl√© 'metadata' vs 'plans_json' selon ton indexeur
        
        # Logique de match (si aucun filtre, on garde tout)
        is_match = False
        badge = ""

        if target_aliases is None:  
            is_match = True
        else:
            for p in plans:
                # V√©rifie section et niveau
                sec = p.get('section', '').lower()
                lvl = p.get('level', '').lower()
                
                # V√©rifie si l'une des sections cibles est pr√©sente
                sec_match = any(alias.lower() in sec for alias in target_aliases)
                lvl_match = target_level.lower() in lvl
                
                if sec_match and lvl_match:
                    is_match = True
                    badge = "Obligatoire" if p.get('isMandatory') else "Optionnel"
                    break
        
        if is_match:
            filtered_candidates.append({
                "id": cid,
                "content": all_data['documents'][idx],
                "meta": meta,
                "badge": badge
            })

    # 3. RERANKING (Classement fin)
    if not filtered_candidates:
        return []

    # On pr√©pare les paires [Query, Document] pour le CrossEncoder
    pairs = [[query, c['content']] for c in filtered_candidates]
    scores = reranker.predict(pairs)

    # On trie par score d√©croissant
    final_results = []
    for i, score in enumerate(scores):
        res = filtered_candidates[i]
        res['score'] = score
        final_results.append(res)
    
    final_results.sort(key=lambda x: x['score'], reverse=True)
    
    return final_results[:top_k]


# ... (Imports et PROGRAMMES dict ici) ...

def main():
    st.title("üéì EPFL Course Matcher")
    
    # Sidebar
    with st.sidebar:
        st.header("Filtres")
        prog_name = st.selectbox("Programme", list(PROGRAMMES.keys()))
        filters = PROGRAMMES[prog_name]
        k = st.slider("Nombre de r√©sultats", 5, 50, 10)

    # Chargement (Une seule fois gr√¢ce au cache)
    emb, rerank, col, bm25, data = load_resources()

    # Input
    query = st.text_area("D√©cris ton job de r√™ve ou tes int√©r√™ts :", height=100)

    if st.button("üöÄ Rechercher"):
        if not query:
            st.warning("√âcris quelque chose !")
            return
        
        with st.spinner("Analyse en cours..."):
            results = search_courses(query, filters, emb, rerank, col, bm25, data, k)
        
        st.success(f"{len(results)} cours trouv√©s")
        
        for r in results:
            score_percent = 1 / (1 + math.exp(-(r['score'] + 6))) # Sigmoid simple
            color = "red" if "bligatoire" in r['badge'] else "green"
            
            st.markdown(f"### [{r['meta']['title']}]({r['meta']['url']})")
            st.caption(f":{color}[{r['badge']}] ‚Ä¢ Pertinence : {score_percent:.0%}")
            st.progress(score_percent)
            with st.expander("Voir l'extrait"):
                st.write(r['content'][:400] + "...")
            st.divider()

if __name__ == "__main__":
    main()